# Sentiment Analysis
We trained a sentiment classifier from tokenized sequences. Our dataloader pads each batch to a common length, the model turns token IDs into embeddings, and then predicts one of three classes. We monitor macro F1 so that each class counts equally, save the best model during training, print a full report with a confusion matrix, and export predictions for the public set.

Our training setup aims for fast and stable learning. We use AdamW with OneCycleLR to speed convergence, dropout and gradient clipping to control overfitting and instability, and class weighting in CrossEntropyLoss together with a gentle WeightedRandomSampler to address skewed labels. Using macro F1 keeps the evaluation honest when classes are imbalanced.

We try two architectures because they capture different signals. TextCNN acts like a bank of phrase detectors with kernel sizes that catch short n grams such as not good or very happy. Global max pooling preserves the strongest activation, which helps when a key phrase appears only once, and the model is fast and strong for short texts. BiLSTM reads both left to right and right to left, which helps with ordering, negation, and longer context such as I thought it would be great but it was not. Our mean and max pooling over time summarizes the sequence without adding an attention layer, which keeps it simple and robust.

Class imbalance is handled in a principled way even if it does not fully solve the problem. We compute inverse frequency class weights and cap the neutral class so it does not dominate the loss. The sampler slightly favors minority examples without oversampling them too aggressively. With macro F1 as the main score we avoid being fooled by high overall accuracy that hides poor minority recall. In short, CNN focuses on local phrases, BiLSTM captures sequence logic, and our loss, sampling, and metric choices work together to push performance on the rare classes, even if the dataset remains challenging.
